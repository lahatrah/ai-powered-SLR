{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import re\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"authentification.json\",\"r\") as f:\n",
    "    AUTH_KEYS = json.load(f)\n",
    "\n",
    "with open(\"prompts.json\",\"r\") as f:\n",
    "    PROMPTS = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-instruct\"\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(model_name,token=AUTH_KEYS[\"hf_token\"])\n",
    "MODEL = AutoModelForCausalLM.from_pretrained(model_name,token=AUTH_KEYS[\"hf_token\"],device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(text):\n",
    "    # Regex pattern to match JSON objects\n",
    "    json_pattern = r'\\{[^{}]*\\}'\n",
    "    matches = re.findall(json_pattern, text)\n",
    "    \n",
    "    valid_jsons = []\n",
    "    for match in matches:\n",
    "        try:\n",
    "            # Attempt to parse the JSON\n",
    "            match = re.sub(r'(\":\\s*)([A-Za-z][^\",\\n}]*)', r'\\1\"\\2\"', match)\n",
    "            json_obj = json.loads(match)\n",
    "            valid_jsons.append(json_obj)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "    \n",
    "    return valid_jsons\n",
    "\n",
    "\n",
    "def extract_summary(text_arg): \n",
    "    return text_arg.split(\"YOUR SUMMARY:\", 1)[-1].strip()\n",
    "\n",
    "\n",
    "def extract_output(text_arg): \n",
    "    return text_arg.split(\"YOUR OUTPUT:\", 1)[-1].strip()\n",
    "\n",
    "def summarize(chunk,device=DEVICE,tokenizer=TOKENIZER,model=MODEL):\n",
    "    prompt_template = PROMPTS[\"summary_prompt\"]\n",
    "    prompt = prompt_template.format(text=chunk[\"text\"])\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device) \n",
    "    outputs = model.generate(**inputs,max_new_tokens=512,do_sample=True,temperature=1e-12,top_k=50)\n",
    "    output = tokenizer.decode(outputs[0], skip_special_tokens=True,past_key_values=None)\n",
    "    output = extract_summary(output)\n",
    "    return output\n",
    "\n",
    "def structure(summary,device=DEVICE,tokenizer=TOKENIZER,model=MODEL):\n",
    "    prompt_template = PROMPTS[\"structuration_prompt\"]\n",
    "    prompt = prompt_template.format(summary=summary)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)  \n",
    "    outputs = model.generate(**inputs, max_new_tokens=512,do_sample=True,temperature=1e-12,top_k=50)\n",
    "    output = tokenizer.decode(outputs[0], skip_special_tokens=True,past_key_values=None)\n",
    "    output = extract_output(output)\n",
    "    output = extract_json(output)\n",
    "    return output\n",
    "\n",
    "def extract_data(full_texts,output_file=None):\n",
    "    columns = [\"vector\",\"location\",\"country\",\"date\",\"year\",\"source_type\"]\n",
    "    counter = 0\n",
    "    for chunk in tqdm(full_texts):\n",
    "        try:\n",
    "            TOKENIZER.truncation_side = \"left\"\n",
    "            summary = summarize(chunk)\n",
    "            output = structure(summary) \n",
    "            counter += 1\n",
    "            if output:  # Only append valid JSON data\n",
    "                data = []\n",
    "                for item in output:\n",
    "                    item[\"source_type\"] = chunk[\"source_type\"]\n",
    "                    item_data = dict((k, item[k]) for k in columns if k in item)\n",
    "                    data.append(item_data)\n",
    "                df = pd.DataFrame(data)\n",
    "                df.to_csv(output_file, mode=\"a\", index=False, header=not counter > 1)   \n",
    "        except RuntimeError as e:  # OutOfMemoryError \n",
    "            if 'out of memory' in str(e).lower():\n",
    "                print(f\"Out of memory error on chunk {chunk.get('document_id', 'unknown')}, skipping...\")\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                continue\n",
    "            else:\n",
    "                raise         \n",
    "        except Exception as e:  \n",
    "            print(f\"Error processing chunk {chunk.get('document_id', 'unknown')}: {str(e)}, skipping...\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"input.json\",\"r\") as f:\n",
    "    inputs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_data(inputs,\"output_dataset.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
